{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "___\n",
    "<h1> Machine Learning </h1>\n",
    "<h2> M. Sc. in Electrical and Computer Engineering </h2>\n",
    "<h3> Instituto Superior de Engenharia / Universidade do Algarve </h3>\n",
    "\n",
    "[MEEC](https://ise.ualg.pt/en/curso/1477) / [ISE](https://ise.ualg.pt) / [UAlg](https://www.ualg.pt)\n",
    "\n",
    "Pedro J. S. Cardoso (pcardoso@ualg.pt)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ANN with Sklearn\n",
    "\n",
    "SKlearn is a simple and efficient tool for data analysis and modeling. It provides a range of supervised and unsupervised learning algorithms through a consistent interface in Python. In particular, it provides a set of tools for neural networks.\n",
    "\n",
    "We are going to consider the following neural network models:\n",
    "- **Perceptron**\n",
    "- **Multi-layer Perceptron (MLP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perceptron\n",
    "\n",
    "The **Perceptron is a simple neural network model that learns the weights of the input features to make a binary classification**. The Perceptron is a single-layer neural network with a **step activation function**. In the Sklearn library, the Perceptron model is available in the linear_model module - sklearn.linear_model.Perceptron - and is used for binary classification problems (two classes). It does not allow for activation functions other than the step function.\n",
    "\n",
    "Although the Perceptron is a simple model, it is the basis for more complex models, such as the Multi-layer Perceptron (MLP). \n",
    "\n",
    "So, let us load the necessary libraries and make an example with the Iris dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The original iris dataset has three classes. We are going to consider only two classes. So, we are going to consider only the first class (labled = 0) and the other two classes (versicolor and virginica) as a single class. The problem is therefore a binary classification problem, where the target variable is True if the class is 0 and False otherwise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target == 0\n",
    "y"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next step we are going to split the data into training and test sets and normalize it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    stratify=iris.target,\n",
    "                                                    random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we are going to train the Perceptron model and predict the target values over the test data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "perceptron = Perceptron(random_state=1,\n",
    "                        max_iter=1000,\n",
    "                        tol=1e-50,\n",
    "                        verbose=True).fit(X_train_scaled, y_train)\n",
    "\n",
    "perceptron.score(X_test_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not bad for a simple model! The Perceptron model is a linear model, so it is not able to learn complex patterns. However, it is a good starting point for more complex models, such as the Multi-layer Perceptron (MLP).\n",
    "\n",
    " Let us see the weights of the input features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "perceptron.coef_"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron (MLP) Classification: Iris dataset\n",
    "In this section, we'll use the Iris dataset to make out first example.\n",
    "\n",
    "So, load and split data:"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n",
    "                                                    stratify=iris.target,\n",
    "                                                    random_state=1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a multi-layer perceptron classifier (MLPClassifier), train and get the score over the test data\n",
    "\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "\n",
    "The default parameters are:\n",
    "- hidden_layer_sizes=(100,)\n",
    "- activation='relu'\n",
    "- solver='adam'\n",
    "- alpha=0.0001\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(verbose=True,  # uncomment to see loss function evolution\n",
    "                    random_state=1\n",
    "                    ).fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "print(f'Final score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "You probably got a warning message related to the number of iterations. The default number of iterations is 200. The warning message is due to the fact that the optimization algorithm did not converge. \n",
    "\n",
    "ok...!? let us see if we can improve this... by increasing the number of iterations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(max_iter=1000,\n",
    "                    random_state=1,\n",
    "                    verbose=True\n",
    "                    ).fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "print(f'Final score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was good! Maybe there were other alternatives, like using more layers...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100),\n",
    "                    random_state=1,\n",
    "                    max_iter=1000,\n",
    "                    verbose=True\n",
    "                    ).fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "print(f'Final score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being a __multi-class classification problem__, the output layer has three neurons (one for each class: setosa, versicolor, and virginica). The __activation function is a softmax function__, which is a generalization of the logistic function to multiple dimensions. \n",
    "\n",
    "The sotmax function allows us to interpret the output of the network as probabilities. The class with the highest probability is the predicted class. The probabilities associated to each test instance are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us see the distribution of the probabilities, for each class, over the test data. This allows us to understand the confidence of the classifier in its predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clf.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-layer Perceptron (MLP) Regression: Bike Sharing dataset\n",
    "\n",
    "In this section, we'll use the Bike Sharing dataset to make a regression example. This dataset contains the hourly count of rental bikes between years 2017 and 2018 in Seoul, Korea. The goal is to predict the number of bikes rented in a given hour based on the features available."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/SeoulBikeData.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the dataset contains both numerical and categorical data. The target variable is the number of rented bikes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we pre-process the data:\n",
    "- transform the date into a datetime object and extract the hour\n",
    "- replacing the categorical data by one-hot encoding and normalizing the numerical data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "df['hour'] = df['Date'].dt.hour\n",
    "df['month'] = df['Date'].dt.month\n",
    "df.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Seasons', 'Holiday', 'Functioning Day'])\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we split the data into features and target and normalize it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_Seoul = df.drop(columns=['Rented Bike Count'])\n",
    "y_Seoul = df['Rented Bike Count']"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_Seoul_scaled = scaler.fit_transform(X_Seoul)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split the data into training and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_Seoul_scaled, y_Seoul,\n",
    "                                                    test_size=0.8,\n",
    "                                                    random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare a multi-layer perceptron regressor (MLPRegressor), train and get the score over the test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = MLPRegressor(max_iter=200,\n",
    "                     random_state=1,\n",
    "                     verbose=True\n",
    "                     ).fit(X_train, y_train)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "print(f'Final score (R2): {score}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You probably got a warning message related to the number of iterations. The default number of iterations is 200. The warning message is due to the fact that the optimization algorithm did not converge. So, let us increase the number of iterations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = MLPRegressor(max_iter=20000,\n",
    "                     random_state=1,\n",
    "                     verbose=True\n",
    "                     ).fit(X_train, y_train)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "print(f'Final score (R2): {score}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's better... but maybe we can improve this... by increasing the number of hidden layers and neurons.\n",
    "The previous model has only one hidden layer with 100 neurons. We can try to improve the model by adding more layers and neurons, for example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = MLPRegressor(max_iter=2000,\n",
    "                     hidden_layer_sizes=(100, 100, 100),\n",
    "                     random_state=1,\n",
    "                     verbose=True\n",
    "                     ).fit(X_train, y_train)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "print(f'Final score (R2): {score}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "A slightly better result was obtained. We can try to improve the model by adding more layers and neurons... or we can use a different activation function. The default activation function is the ReLU function. Try it...!\n",
    "\n",
    "To compare the predicted values with the true values we can plot them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "plt.plot(y_test, y_pred, 'o')\n",
    "plt.plot([0,2500], [0,2500], '-', label='test')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ANN with Keras\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "Keras wraps the efficient numerical computation libraries Theano and TensorFlow and allows you to define and train neural network models in just a few lines of code.\n",
    "\n",
    "In this section, we'll use Keras to build a neural network to classify the digits dataset and to predict the number of bikes rented in a given hour based on the Bike Sharing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tf.random.set_seed(42)      # Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification: Digits dataset\n",
    "\n",
    "In this section, we'll use Keras to build a neural network to classify the digits dataset. The digits dataset consists of 8x8 pixel images of digits. The goal is to classify the images into one of ten classes (0-9). \n",
    "\n",
    "First, we'll load the data, normalize it, and split it into training, validation, and test sets. Then, we'll build a neural network with a simple architecture and train it on the training data. Finally, we'll evaluate the model on the test data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = load_digits()      # We are using the digits dataset, from sklearn.datasets \n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Normalize data - values are between 0 and 16\n",
    "X = X / 16.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "One-hot encode the labels: need because we are using a softmax activation function, which expects one-hot encoded labels - the output layer has 10 neurons (one for each digit)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y = to_categorical(y)\n",
    "y[:5]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split data into training, validation, and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test,\n",
    "                                                test_size=0.5,\n",
    "                                                random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, let us build the neural network. We'll use a simple architecture with:\n",
    "- an input layer with 64 neurons (one for each pixel in the image),\n",
    "- two hidden layers, each with 100 neurons, and ReLU activation functions,\n",
    "- an output layer with 10 neurons (one for each digit) and a softmax activation function.\n",
    "- a dropout layer to avoid overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)                      # Set random seed for reproducibility\n",
    "\n",
    "m, n = X_train.shape\n",
    "\n",
    "model = Sequential()                        # Sequential model - a linear stack of layers\n",
    "\n",
    "model.add(Input(shape=(n,)))                # Input layer with 64 neurons\n",
    "model.add(Dense(100, activation='relu'))    # Hidden layer with 100 neurons and ReLU activation function\n",
    "model.add(Dropout(0.1))                     # Dropout layer with 10% of neurons being dropped\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))  # 10 neurons, one for each digit and softmax activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the next step, we'll compile the model, specifying the optimizer, loss function, and metrics to be used during training. Compiling the model means that Keras will prepare the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # Cross-entropy loss function - used for multi-class classification problems\n",
    "              metrics=['accuracy', 'f1_score']  # Accuracy and F1 score metrics\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The shapes of the input and output layers are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A plot of the architecture is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=40, \n",
    "          batch_size=32,\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We achieved an accuracy of 0.9815 on the validation data and a f1-score of 0.9809. Let us evaluate the model on the test data, and see that we get similar results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test loss: {res[0]:.4f}')\n",
    "print(f'Test accuracy: {res[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regression: Bike Sharing dataset\n",
    "\n",
    "In this section, we'll use Keras to build a neural network to predict the number of bikes rented in a given hour based on the Bike Sharing dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/SeoulBikeData.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we prepossess the data:\n",
    "- transform the date into a datetime object and extract the hour\n",
    "- replacing the categorical data by one-hot encoding and normalizing the numerical data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "df['hour'] = df['Date'].dt.hour\n",
    "df['month'] = df['Date'].dt.month\n",
    "df.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Seasons', 'Holiday', 'Functioning Day'])\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we split the data into features and target and normalize it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Rented Bike Count'])\n",
    "y = df['Rented Bike Count']"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We normalize the data using the StandardScaler from sklearn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split the data into training, validation, and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test,\n",
    "                                                test_size=0.5,\n",
    "                                                random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us build the neural network. We'll use a simple architecture with:\n",
    "- an input layer with 19 neurons (one for each feature),\n",
    "- three hidden layers, each with 100 neurons, and ReLU activation functions,\n",
    "- an output layer with one neuron and a linear activation function.\n",
    "- a dropout layer to avoid overfitting\n",
    "- the loss function is the mean squared error\n",
    "- the metric is the mean absolute error\n",
    "- the optimizer is the Adam optimization algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "tf.random.set_seed(42)                      # Set random seed for reproducibility\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(n,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',                   # Adam optimization, a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "    loss='mean_squared_error',          # Mean squared error loss function\n",
    "    metrics=['mean_absolute_error']     # Mean absolute error metric\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=1000, \n",
    "          batch_size=32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us see the loss evolution during training and check if there was overfitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "history = model.history\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Evolution During Training')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, we can see that after ~250 epochs the validation stop decreasing (maybe increasing) and the training loss continues to decrease. This is a sign of overfitting. We can stop the training at this point.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)                      # Set random seed for reproducibility\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=250,\n",
    "          batch_size=32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us evaluate the model on the test data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "res = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {res[0]:.4f}')\n",
    "print(f'MSE: {res[1]:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "If wanted, we can compute the R² score, using the sklearn.metrics.r2_score function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R²: {r2:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now, we can plot the predicted values against the true values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(y_test, y_pred, 'o')\n",
    "plt.plot([0,2500], [0,2500], '-', label='test')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dropout, L1, and L2 Regularization\n",
    "\n",
    "Dropout, L1, and L2 regularization are techniques used to prevent overfitting in neural networks. We already saw the Dropout technique in the previous examples. Let us see the L1 and L2 regularization techniques."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(n,)))\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer='l1'))  # L1 regularization\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer='l2'))  # L2 regularization\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer='l1l2'))  # L1 and L2 regularization\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=400,\n",
    "          batch_size=32)\n",
    "\n",
    "res = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {res[0]:.4f}')\n",
    "print(f'MSE: {res[1]:.4f}')\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R²: {r2:.4f}')\n",
    "\n",
    "plt.plot(y_test, y_pred, 'o')\n",
    "plt.plot([0,2500], [0,2500], '-', label='test')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
