{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "___\n",
    "<h1> Machine Learning </h1>\n",
    "<h2> M. Sc. in Electrical and Computer Engineering </h2>\n",
    "<h3> Instituto Superior de Engenharia / Universidade do Algarve </h3>\n",
    "\n",
    "[MEEC](https://ise.ualg.pt/en/curso/1477) / [ISE](https://ise.ualg.pt) / [UAlg](https://www.ualg.pt)\n",
    "\n",
    "Pedro J. S. Cardoso (pcardoso@ualg.pt)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Feature engineering\n",
    "\n",
    "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself. Examples of feature engineering include:\n",
    "- deriving new features from existing data,\n",
    "- selecting only the most relevant features,\n",
    "- creating features from images, text, and sensor data,\n",
    "- normalizing numerical features,\n",
    "- encoding categorical features,\n",
    "- transforming features into a more suitable format for machine learning algorithms.\n",
    "- ...\n",
    "\n",
    "In this notebook, we will explore some of these techniques. For that, let us consider the Seoul Bike Sharing Demand dataset. The dataset contains the hourly count of rental bikes between years 2017 and 2018 in Seoul, Korea with the corresponding weather and seasonal information. The dataset can be downloaded from https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand but we have already downloaded it and saved it in the `data` folder.\n",
    "\n",
    "So, we can start by loading the dataset into a pandas dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/SeoulBikeData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "By calling the dataframe's `info` method, we can see that there are no missing values but there are some categorical columns.\n",
    "(For treating missing values, please refer to the `19-Missing-Data.ipynb` notebook were some techiniques are studied.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Categorical data transformation\n",
    "\n",
    "Most machine learning algorithms cannot handle categorical data. Therefore, categorical data must be transformed into numerical data. There are several ways to do this, like:\n",
    "- One-hot encoding -- transform each category into a binary column\n",
    "- Ordinal encoding -- transform each category into a number\n",
    "- Binary encoding -- transform each category into a binary number\n",
    "- Hash encoding -- transform each category into a hash number\n",
    "- ...\n",
    "\n",
    "Let us see how to performe the first two techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### One hot encoding\n",
    "\n",
    "One hot encoding is a technique used to transform categorical features to binary features. The idea is to create a new column for each category and assign a 1 or 0 to the column. For example, the season column has four categories: Spring, Summer, Autumn, and Winter. We can convert this column into four columns, one for each category, and use 0 or 1 to indicate if the sample belongs to that category or not. To achieve this, we can use the pandas get_dummies method (we'll do it for the 'Holiday' and 'Functioning Day' columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['Holiday', 'Functioning Day'], drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Ordinal encoding\n",
    "Ordinal encoding is a technique used to transform categorical features to ordinal features. The idea is to assign a number to each category. For example, the season column has four categories: Spring, Summer, Autumn, and Winter. We can convert this column into a single column with values 1, 2, 3, and 4. To achieve this, we can use the pandas replace method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Seasons'] = df['Seasons'].replace({'Spring': 1, 'Summer': 2, 'Autumn': 3, 'Winter': 4})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dates transformation\n",
    "\n",
    "Dates are usually represented as strings. However, machine learning algorithms cannot handle strings. Therefore, dates must be transformed into numerical data. There are several ways to do this, like extracting the year, month, day, day of week etc. from the date\n",
    "\n",
    "In our case, we can split this column into two columns: month and day, and day of week. To achieve this, we can use the pandas to_datetime method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure the date column is in datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# create new columns for month, day, and day of week\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day'] = df['Date'].dt.day\n",
    "df['day_of_week'] = df['Date'].dt.day_of_week\n",
    "\n",
    "# drop the original date column\n",
    "df.drop('Date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let us now recheck the dataframe's info method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We should be now able to apply machine learning algorithms to this dataset. However, we can still improve the performance of the algorithms by applying some feature engineering techniques. But let us see how the algorithms perform without any feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_X_and_y(df):\n",
    "    \"\"\" Returns the features and the target variable\"\"\"\n",
    "    X = df.drop(['Rented Bike Count'], axis=1)\n",
    "    y = df['Rented Bike Count']\n",
    "    return X, y\n",
    "\n",
    "def run(df):\n",
    "    \"\"\" Runs the models (LinearRegression, Ridge, Lasso, SVR, KNeighborsRegressor, RandomForestRegressor, MLPRegressor) \n",
    "    and returns the scores\"\"\"\n",
    "    # get X and y\n",
    "    X, y = get_X_and_y(df)\n",
    "    \n",
    "    # split the data into train and test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, shuffle=True)\n",
    "\n",
    "    models = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'Ridge': Ridge(),\n",
    "        'Lasso': Lasso(),\n",
    "        'SVR': NuSVR(),\n",
    "        'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "        'RandomForestRegressor': RandomForestRegressor(),\n",
    "        #'MLPRegressor': MLPRegressor(max_iter=10000)\n",
    "    }\n",
    " \n",
    "    fig, ax = plt.subplots(len(models), 1, figsize=(10, 40))\n",
    "    scores = {}\n",
    "    for idx, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        pred = model.predict(X_test)\n",
    "        print(f'{name}: score = {score}')\n",
    "\n",
    "        # plot pred vs actual\n",
    "        ax[idx].plot(y_test.values, pred, c='g', marker='o', linestyle='None')\n",
    "        ax[idx].plot(y_test.values, y_test.values, c='r')\n",
    "        ax[idx].set_ylabel('Predicted')\n",
    "        ax[idx].set_xlabel('Actual')\n",
    "        ax[idx].set_title(f'{name} / Score =  {score}')   \n",
    "        \n",
    "        scores[name] = score\n",
    "    \n",
    "    return scores "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us run the models without any extra feature engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_scores = pd.DataFrame()\n",
    "all_scores['without scaling or poly'] = run(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Feature scaling\n",
    "Feature scaling is the process of transforming numerical features to a common scale. There are several ways to do this, like:\n",
    "- Normalization -- transform each feature to a range between 0 and 1\n",
    "- Standardization -- transform each feature to a normal distribution with mean 0 and standard deviation 1\n",
    "- ...\n",
    "\n",
    "The original dataset has the following distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Box plots also help with visualization of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop(['Rented Bike Count'], axis=1).plot(kind='box', figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Standardization (or Z-score normalization)\n",
    "\n",
    "Standardization is a technique used to transform numerical features to a normal distribution with mean 0 and standard deviation 1. The idea is to subtract the mean and divide by the standard deviation. The formula is given by\n",
    "$$ X'_{ij} = \\frac{X_{ij}-\\mu_j}{\\sigma_j}$$\n",
    "where $X_{ij}$ is the observation $i$ for the feature $j$, $\\mu_j$ is the mean and $\\sigma_j$ is the standard deviation.\n",
    "\n",
    "\n",
    "To achieve this, we can use the pandas mean and std methods or call the sklearn StandardScaler method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let us now apply the standardization technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# get X and y\n",
    "X, y = get_X_and_y(df)\n",
    "\n",
    "# set and fit the scaler\n",
    "standard_scaler = StandardScaler().fit(X)\n",
    "\n",
    "# normalize the data\n",
    "df_std = pd.DataFrame(standard_scaler.transform(X), columns = X.columns)\n",
    "df_std.plot(kind='box', figsize=(20,5))\n",
    "\n",
    "df_std['Rented Bike Count'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So, let us create a model but now using the standarderized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_scores['with standardization'] =  run(df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### MinMaxScaler\n",
    "\n",
    "Another usual solution is to normalize the distribution by subtracting the minimum and dividing by the difference between the maximum and the minimum,\n",
    "\n",
    "$$ X'_{ij} = \\frac{X_{ij}-\\min_j}{\\max_j-\\min_j}$$\n",
    "\n",
    "where X_{ij} is the observation $i$ for the feature $j$, $\\min_j$ is the minimum and $\\max_j$ is the maximum. Returned values are in the range [0, 1].\n",
    "\n",
    "This can be done by coding or simply using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X, y = get_X_and_y(df)\n",
    "\n",
    "# set and fit the scaler\n",
    "minmax_scaler = MinMaxScaler().fit(X)\n",
    "\n",
    "df_minmax = pd.DataFrame(minmax_scaler.transform(X), columns = X.columns)\n",
    "df_minmax.plot(kind='box', figsize=(20,5))\n",
    "\n",
    "df_minmax['Rented Bike Count'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So, let us create a model but now using the scaled data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_scores['with minmax'] = run(df_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Polynomial features\n",
    "\n",
    "Other approach is to create polynomial features. In this case, if the original set of feature is $(x_1, x_2, x_n)$ then the polynomial features with degree 2 are $(1, x_1, x_2, x_n, x_1^2, x_1x_2, x_1x_n, x_2^2, x_2x_n, ...,  x_n^2 \\ldots)$.\n",
    "\n",
    "This can be done by coding or simply using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = get_X_and_y(df)\n",
    "\n",
    "# set and fit the scaler\n",
    "poly = PolynomialFeatures(degree=2).fit(X)\n",
    "\n",
    "df_poly = pd.DataFrame(poly.transform(X), columns = poly.get_feature_names_out(X.columns))\n",
    "\n",
    "df_poly.plot(kind='box', figsize=(20,5))\n",
    "\n",
    "df_poly['Rented Bike Count'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train a model using the polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_scores['with poly'] = run(df_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Normalization + Polynomial features\n",
    "\n",
    "Now, let us combine both normalization and polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get X and y\n",
    "X, y = get_X_and_y(df)\n",
    "\n",
    "# set and fit the scaler\n",
    "standard_scaler = StandardScaler().fit(X)\n",
    "\n",
    "# normalize the data\n",
    "df_std = pd.DataFrame(standard_scaler.transform(X), columns = X.columns)\n",
    "\n",
    "# set and fit the scaler\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False).fit(df_std)\n",
    "\n",
    "df_std_poly = pd.DataFrame(poly.transform(df_std), columns = poly.get_feature_names_out(df_std.columns))\n",
    "\n",
    "df_std_poly.plot(kind='box', figsize=(20,5))\n",
    "\n",
    "df_std_poly['Rented Bike Count'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_scores['with standardization and poly'] =  run(df_std_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_scores.plot(figsize=(20,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Log transformation\n",
    "\n",
    "Since the target variable is supposed to be a count ($\\geq 0$), we can apply a log transformation to it (sum $\\delta$ to avoid any log(0)). After, we can train the models and then apply the inverse transformation to the predictions, i.e., if $y$ is the target variable, we can train the models using $\\log(y+\\delta)$ and then apply the inverse transformation to the predictions, i.e., if $\\hat{y}$ is the prediction, then the final prediction is $\\exp(\\hat{y})-\\delta$. The $\\delta$ value can be chosen by the user, but it is usually set to a small value such as 1, avoiding the log(0) problem.\n",
    "\n",
    "Note that, in this case, the predictions will be positive numbers. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def run_logged(df):\n",
    "    \"\"\" Runs the models (LinearRegression, Ridge, Lasso, SVR, KNeighborsRegressor, RandomForestRegressor, MLPRegressor) \n",
    "    and returns the scores\"\"\"\n",
    "    # get X and y\n",
    "    delta = 1\n",
    "    X, y = get_X_and_y(df)\n",
    "    y = np.log(y+delta)\n",
    "\n",
    "    # split the data into train and test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=41, shuffle=True)\n",
    "\n",
    "    models = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'Ridge': Ridge(),\n",
    "        'Lasso': Lasso(),\n",
    "        'SVR': NuSVR(),\n",
    "        'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "        'RandomForestRegressor': RandomForestRegressor(),\n",
    "        #'MLPRegressor': MLPRegressor(max_iter=10000)\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(len(models), 1, figsize=(10, 40))\n",
    "    scores = {}\n",
    "    for idx, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        pred = model.predict(X_test)\n",
    "        pred = np.exp(pred) - delta\n",
    "        \n",
    "        r2_score_inverse_logged = r2_score(np.exp(y_test.values)-delta, pred)\n",
    "        \n",
    "        print(f'{name}: score logged = {score} / r2_score inverse logged = {r2_score_inverse_logged}')\n",
    "\n",
    "        # plot pred vs actual\n",
    "        ax[idx].plot(np.exp(y_test.values)-delta, pred, c='g', marker='o', linestyle='None')\n",
    "        ax[idx].plot(np.exp(y_test.values)-delta, np.exp(y_test.values)-delta, c='r')\n",
    "        ax[idx].set_ylabel('Predicted')\n",
    "        ax[idx].set_xlabel('Actual')\n",
    "        ax[idx].set_title(f'{name} / Score =  {r2_score_inverse_logged}')\n",
    "\n",
    "        scores[name] = score\n",
    "\n",
    "    return scores "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get X and y\n",
    "X, y = get_X_and_y(df)\n",
    "\n",
    "# set and fit the scaler\n",
    "standard_scaler = StandardScaler().fit(X)\n",
    "\n",
    "# normalize the data\n",
    "df_std = pd.DataFrame(standard_scaler.transform(X), columns = X.columns)\n",
    "\n",
    "# set and fit the scaler\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False).fit(df_std)\n",
    "\n",
    "df_std_poly = pd.DataFrame(poly.transform(df_std), columns = poly.get_feature_names_out(df_std.columns))\n",
    "\n",
    "df_std_poly.plot(kind='box', figsize=(20,5))\n",
    "\n",
    "df_std_poly['Rented Bike Count'] = y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_logged(df_std_poly) "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusions\n",
    "\n",
    "We can see that, except for the RF, all models perform better when we apply feature engineering techniques. Also note that no hyperparameter tuning was performed. So, we can expect even better results if we tune the hyperparameters of each model. This would be a good exercise for you to do... but, be careful, it can be very time consuming for some models such as the MLPRegressor or SVM. "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
