{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "<h1> Machine Learning </h1>\n",
    "<h2> M. Sc. in Electrical and Computer Engineering </h2>\n",
    "<h3> Instituto Superior de Engenharia / Universidade do Algarve </h3>\n",
    "\n",
    "[MEEC](https://ise.ualg.pt/en/curso/1477) / [ISE](https://ise.ualg.pt) / [UAlg](https://www.ualg.pt)\n",
    "\n",
    "Pedro J. S. Cardoso (pcardoso@ualg.pt)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups, fetch_20newsgroups_vectorized\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk.stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "In this notebook we'll be doing some clustering over text. So we'll start by seeing how to convert text to something more easy to cluster... vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bags of Words\n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval. \n",
    "\n",
    "In this model, a text (such as a sentence or a document) is represented as the **bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity**. (wikipedia, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences = [\n",
    "    \"\"\"Space: the final frontier. These are the voyages of the starship Enterprise.\n",
    "        Its five-year mission: to explore strange new worlds. To seek out new life and new civilizations. To boldly go where no man has gone before!\"\"\",\n",
    "    \"Help me, Obi-Wan Kenobi. You’re my only hope.\",\n",
    "    \"I find your lack of faith disturbing\",\n",
    "    \"\"\"It’s the ship that made the Kessel run in less than twelve parsecs. I’ve outrun Imperial starships. Not the local bulk cruisers, mind you. \n",
    "        I’m talking about the big Corellian ships, now. She’s fast enough for you, old man\"\"\",\n",
    "    \"The Force will be with you. Always\",\n",
    "    \"Never tell me the odds\",\n",
    "    \"No. I am your father\"\n",
    "]\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "\n",
    "Let us convert a collection of text documents to a ** (sparce) matrix of token counts**\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the **number of features will be equal to the vocabulary size found by analyzing the data**.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = CountVectorizer(\n",
    "    strip_accents=\"unicode\", \n",
    "    lowercase=True, \n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "vectorized.fit(list_of_sentences)\n",
    "vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then do the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = vectorized.transform(list_of_sentences)\n",
    "mx.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative we can learn the vocabulary dictionary and return document-term matrix in a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = vectorized.fit_transform(list_of_sentences)\n",
    "mx.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the feature names are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(vectorized.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is possible to transform documents to document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_be_transformed = [\n",
    "    \"\"\"Frontier is an outer limit in a field of endeavor, especially one in which the opportunities for research and development have not been exploited: \n",
    "    the frontiers of space exploration.\"\"\", \n",
    "    \"\"\"Space: the final frontier. These are the voyages of the starship Enterprise. Its continuing mission: to explore strange new worlds. \n",
    "    To seek out new life and new civilizations. To boldly go where no one has gone before!\"\"\"\n",
    "]\n",
    "vectorized.transform(to_be_transformed).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe there are some problems: what about plural, variations, tense, ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = vectorized.transform([\"Space frontier civilizations worlds\",  \n",
    "                      \"spaces frontiers civilization world\"]).toarray()\n",
    "mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer\n",
    "- A varition is `TfidfVectorizer` - Transform a count matrix to a normalized tf or tf-idf representation. \n",
    "\n",
    "- Note that **_Tf_ means term-frequency** while **_tf-idf_ means term-frequency times inverse document-frequency**. \n",
    "\n",
    "- The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to **scale down the impact of tokens that occur very frequently in a given corpus** and that are hence empirically **less informative than features that occur in a small fraction of the training corpus**.\n",
    "\n",
    "$$tf-idf(t, d) = tf(t, d) \\times idf(t)$$\n",
    "where\n",
    "$$idf(t) = \\log\\left(\\frac{1 + n}{1 + df(t)}\\right) + 1$$\n",
    "and\n",
    "$$tf(t, d) = \\text{number of times term t appears in document d}$$\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = TfidfVectorizer(\n",
    "    strip_accents=\"ascii\", \n",
    "    lowercase=True, \n",
    "    stop_words='english'\n",
    ")\n",
    "vectorized.fit(list_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The transformation of \", to_be_transformed)\n",
    "vectorized.transform(to_be_transformed).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer\n",
    "Snowball Stemmer allows us to create more thoughful bag-of-words by **removing morphological affixes from words**, leaving only the word stem.\n",
    "\n",
    "https://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(english_stemmer.stem, ['civilization', 'civilizations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(english_stemmer.stem, ['jump', 'jumping', 'jumps', 'jumped']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(english_stemmer.stem, ['ship', 'ships', 'shipping', 'shipped']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `TfidfVectorizer` we can extend it and, to apply the snowball stemmer, we redefine its `build_analyzer` (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.build_analyzer), as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    # overriding the build_analyzer\n",
    "    def build_analyzer(self):\n",
    "        '''Return a callable that handles preprocessing, tokenization and n-grams generation.'''\n",
    "        analyzer = super().build_analyzer()\n",
    "        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "\n",
    "steemed_vectorizer = StemmedTfidfVectorizer(stop_words='english', \n",
    "                                            decode_error ='ignore', \n",
    "                                            encoding='utf-8'\n",
    "                                           )\n",
    "X = steemed_vectorizer.fit_transform(list_of_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, given the list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences = [\n",
    "    \"\"\"Space: the final frontier. These are the voyages of the starship Enterprise.\n",
    "        Its five-year mission: to explore strange new worlds. To seek out new life and new civilizations. To boldly go where no man has gone before!\"\"\",\n",
    "    \"Help me, Obi-Wan Kenobi. You’re my only hope.\",\n",
    "    \"I find your lack of faith disturbing\",\n",
    "    \"\"\"It’s the ship that made the Kessel run in less than twelve parsecs. I’ve outrun Imperial starships. Not the local bulk cruisers, mind you. \n",
    "        I’m talking about the big Corellian ships, now. She’s fast enough for you, old man\"\"\",\n",
    "    \"The Force will be with you. Always\",\n",
    "    \"Never tell me the odds\",\n",
    "    \"No. I am your father\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding features are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(steemed_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the previous list of features names was the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorized.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the the Tf-idf-weighted document-term matrix (associated with the learned vocabulary) is a sparse matrix of type '<class 'numpy.float64'> with elements in Compressed Sparse Row format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now, its is possible to apply to new strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The transformation of \", to_be_transformed, \"is \")\n",
    "steemed_vectorizer.transform(to_be_transformed).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newsgroup Clustering\n",
    "## Prepare newsgroups' data\n",
    "\n",
    "Let us start by getting some posts from the 20 newsgroups text dataset. \n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train', \n",
    "    remove=(\"headers\", \"footers\", \"quotes\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(newsgroups_train.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes +11K posts which could be used for classification (predict post's group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement a vectorizer with a stemmer approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    # overriding the build_analyzer\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super().build_analyzer()\n",
    "        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "\n",
    "steemed_vectorizer = StemmedTfidfVectorizer(\n",
    "    stop_words='english', \n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 2), # The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "    min_df=10, # When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "    max_df=.3, # When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. \n",
    "    decode_error ='ignore',  # Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding.\n",
    "    encoding='utf-8',\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "X = steemed_vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag contains +11k features. Some good, others.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(steemed_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(steemed_vectorizer.get_feature_names_out()[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(steemed_vectorizer.get_feature_names_out()[900:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first post, the values in the documento-term matrix are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"For the first post:\\ng\", newsgroups_train.data[0])\n",
    "print(X[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the columns for which the first post (line 0) has values different from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = X[0,:].nonzero()\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now see the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join((steemed_vectorizer.get_feature_names_out()[c] for c in cols))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustring over newsgroup's data - KMeans\n",
    "\n",
    "The newsgroup dataset is labeled and could be used for classification but here we are going to create some clusters. To achieve it we are going to use KMeans with 20 clusters (as many as the groups in the dataset!)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 20\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=num_clusters, \n",
    "    init='k-means++', \n",
    "    n_init=20, \n",
    "    verbose=1)\n",
    "\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The post labels stored in the `labels_` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are not directly comparable with the newsgroups_train target values as labels (0-20) were randomly assigned. \n",
    "So, to make some comparision it would be necessary to map the labels (in a optimum form!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "If we have a new post first we transform it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = '''Disk drive problems.\n",
    " Hi, I have a problem with my hard disk. \n",
    " After 1 year it is working only sporadically now. \n",
    " I tried to format it, but now it doesn't boot any more. Any ideas? Thanks.'''\n",
    "\n",
    "post_vec = steemed_vectorizer.transform([post])\n",
    "print(post_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we send it to the KMeans instance, getting a cluster label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_label = km.predict(post_vec)\n",
    "post_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us get the post with same labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_indices = (km.labels_== post_label[0]).nonzero()[0]\n",
    "similar_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compute which ones are more similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "similar = []\n",
    "for i in similar_indices:\n",
    "    dist = np.linalg.norm((post_vec - X[i]).toarray())\n",
    "    similar.append((dist, newsgroups_train.data[i], i))\n",
    "    \n",
    "similar = sorted(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the most \"similar\" to \n",
    "    \n",
    "    `Disk drive problems.\n",
    "     Hi, I have a problem with my hard disk. \n",
    "     After 1 year it is working only sporadically now. \n",
    "     I tried to format it, but now it doesn't boot any more. Any ideas? Thanks.`\n",
    " \n",
    "is...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(similar[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    " and the least similiar inside the cluster is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(similar[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "256px",
    "width": "473px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
