{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "<h1> Machine Learning </h1>\n",
    "<h2> M. Sc. in Electrical and Computer Engineering </h2>\n",
    "<h3> Instituto Superior de Engenharia / Universidade do Algarve </h3>\n",
    "\n",
    "[MEEC](https://ise.ualg.pt/en/curso/1477) / [ISE](https://ise.ualg.pt) / [UAlg](https://www.ualg.pt)\n",
    "\n",
    "Pedro J. S. Cardoso (pcardoso@ualg.pt)\n",
    "___\n",
    "\n",
    "_Note: running this notebook will, probably, require some hours._ "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines (SVMs) \n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/svm.html for an explanation of the module and https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation for a the mathematical formulation.\n",
    "\n",
    "\n",
    "\n",
    "## Classification\n",
    "\n",
    "Let us start with a simple example of classification using SVM. We will use the iris dataset and, as usual, we will split the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data,\n",
    "                                                    iris.target, \n",
    "                                                    random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us train a SVM classifier using the training set and test it using the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=.1, \n",
    "          kernel='poly', \n",
    "          degree=4).fit(X_train, y_train)\n",
    "\n",
    "score = svm.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Changing C and kernel parameters, we can get better results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(\n",
    "    C=.01, \n",
    "    kernel='poly', \n",
    "    degree=4).fit(X_train, y_train)\n",
    "\n",
    "score = svm.score(X_test, y_test)\n",
    "print(score)\n",
    "print('\"1.0!! Pure luke\"!! try with other random state value (train_test_split)!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html#sphx-glr-auto-examples-svm-plot-iris-svc-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T13:53:16.149339Z",
     "start_time": "2019-05-13T13:53:16.146331Z"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "Next we present a few examples of regression using SVM. \n",
    "\n",
    "Let us consider the Seoul Bike Sharing Demand dataset. The dataset contains the hourly count of rental bikes between years 2017 and 2018 in Seoul, Korea with the corresponding weather and seasonal information. The dataset can be downloaded from https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand but we have already downloaded it and saved it in the data folder.\n",
    "\n",
    "Let us start by loading the dataset into a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/SeoulBikeData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "By calling the dataframe's info method, we can see that there are no missing values but there are some categorical columns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The categorical columns need to be converted into, for example, dummy variables. \n",
    "\n",
    "A dummy variable is a numerical variable used in regression analysis to represent subgroups of the sample in your study. In research design, a dummy variable is often used to distinguish different treatment groups. for example the season column has four categories: Spring, Summer, Autumn, and Winter. We can convert this column into four columns, one for each category, and use 0 or 1 to indicate if the sample belongs to that category or not. To achieve this, we can use the pandas get_dummies method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['Seasons', 'Holiday', 'Functioning Day'], drop_first=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can split this column into two columns: month and day, and day of week. To achieve this, we can use the pandas to_datetime method as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make sure the date column is in datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# create new columns for month, day, and day of week\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day'] = df['Date'].dt.day\n",
    "df['day_of_week'] = df['Date'].dt.day_of_week\n",
    "\n",
    "# drop the original date column\n",
    "df.drop('Date', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us now recheck the dataframe's info method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the target variable is the Rented Bike Count, we can split the dataframe into two dataframes: one with the target variable and another with the remaining variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df.drop('Rented Bike Count', axis=1)\n",
    "y = df['Rented Bike Count']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Following the usual procedure, we can split the dataset into training and test sets. Shuffling the dataset is important to avoid any ordering bias."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To train a SVM regressor, we can use the SVR class from the sklearn.svm module. Furthermore, we can use the GridSearchCV class to perform a grid search to find the best parameters for the SVR model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_model_with_GSCV(X_train, y_train):\n",
    "    grid_search_parameters = [\n",
    "        {'kernel': ['linear'], 'C': [10**i for i in range(-3, 4)]},\n",
    "        {'kernel': ['rbf'], 'C': [10**i for i in range(-3, 4)], 'gamma': [10**i for i in range(-3, 4)]},\n",
    "        {'kernel': ['poly'], 'C': [10**i for i in range(-3, 4)], 'degree': [2, 3]}\n",
    "    ]\n",
    "    \n",
    "    # for a faster search while coding, use the following parameters (if you have time, try the above parameters)\n",
    "    # you can also change the number of folds (cv) to 3\n",
    "    # grid_search_parameters = [\n",
    "    #     {'kernel': ['linear'], 'C': [0.1]},\n",
    "    #     {'kernel': ['rbf'], 'C': [0.1], 'gamma': [0.1]},\n",
    "    #     {'kernel': ['poly'], 'C': [0.1], 'degree': [0.1]}\n",
    "    # ]\n",
    "    \n",
    "    # create the model\n",
    "    svr = SVR()\n",
    "    \n",
    "    # create grid search and fit it to the training data\n",
    "    gs_model = GridSearchCV(estimator=svr,\n",
    "                            param_grid=grid_search_parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1).fit(X_train, y_train)\n",
    "    return gs_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gdcv_model = create_model_with_GSCV(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters and score can be obtained as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gdcv_model.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gdcv_model.best_score_ "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that refit is by default=True, which means that the GridSearchCV will refit an estimator using the best found parameters on the whole dataset. And the best estimator can be obtained as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = gdcv_model.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Over the test set, we can obtain the score as follows, which somehow indicates how well the model generalizes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can make predictions over the test set as follows and compare the predicted values with the actual values, by plotting them. On a prefect regression, the points would be on the diagonal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make predictions over the test set\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# plot pred vs actual\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(y_test.values, pred, c='g', marker='o', linestyle='None')\n",
    "plt.plot([0,3500], [0, 3500], c='r')\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Actual vs Predicted\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering: SVM on a normalized & extended Seoul Bike Dataset\n",
    "\n",
    "## Normalization\n",
    "Some methods have difficulties to deal with data that are not normalized. Let us see how the Seoul Bike Data is distributed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "|Box plots also help with visualization of the distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.plot(kind='box', figsize=(20,5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Standardization\n",
    "One usual solution is to normalize the distribution by subtracting the mean and dividing by the standard deviation, \n",
    "\n",
    "$$X'_{ij} = \\frac{X_{ij}-\\mu_j}{\\sigma_j}$$\n",
    "\n",
    "where X_{ij} is the observation $i$ for the feature $j$, $\\mu_j$ is the mean and $\\sigma_j$ is the standard deviation.\n",
    "\n",
    "This can be done by coding or simply using sklearn "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# set and fit the scaler\n",
    "standard_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_normalized = pd.DataFrame(standard_scaler.transform(X_train), columns = X_train.columns)\n",
    "X_train_normalized.plot(kind='box', figsize=(20,5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, let us create a model but now using the standarderized data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gdcv_normalized_model = create_model_with_GSCV(X_train_normalized, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters and score can be obtained as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gdcv_normalized_model.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best score is"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gdcv_normalized_model.best_score_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that refit is by default=True, which means that the GridSearchCV will refit an estimator using the best found parameters on the whole dataset. And the best estimator can be obtained as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_normalized = gdcv_normalized_model.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Over the test set, we can obtain the score as follows, which somehow indicates how well the model generalizes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_normalized = pd.DataFrame(standard_scaler.transform(X_test), columns = X_train.columns) \n",
    "pred_normalized = model_normalized.predict(X_test_normalized)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can make predictions over the test set as follows and compare the predicted values with the actual values, by plotting them. On a prefect regression, the points would be on the diagonal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot pred vs actual\n",
    "plt.plot( y_test.values, pred_normalized, c='g', marker='o', linestyle='None')\n",
    "\n",
    "# plot the diagonal\n",
    "plt.plot([0,3500], [0, 3500], c='r')\n",
    "\n",
    "# set labels\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Actual vs Predicted')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MinMaxScaler\n",
    "\n",
    "Another usual solution is to normalize the distribution by subtracting the minimum and dividing by the difference between the maximum and the minimum,\n",
    "\n",
    "$$ X'_{ij} = \\frac{X_{ij}-\\min_j}{\\max_j-\\min_j}$$\n",
    "\n",
    "where X_{ij} is the observation $i$ for the feature $j$, $\\min_j$ is the minimum and $\\max_j$ is the maximum. Returned values are in the range [0, 1].\n",
    "\n",
    "This can be done by coding or simply using sklearn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# set and fit the scaler\n",
    "minmax_scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train_minmax = pd.DataFrame(minmax_scaler.transform(X_train), columns = X_train.columns)\n",
    "X_train_minmax.plot(kind='box', figsize=(20,5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, let us create a model but now using the standarderized data "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gdcv_minmax_model = create_model_with_GSCV(X_train_minmax, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters and score can be obtained as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gdcv_minmax_model.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best score is"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gdcv_minmax_model.best_score_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And, to compare the predictions with the actual values, by plotting them. On a prefect regression, the points would be on the diagonal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make predictions over the test set\n",
    "X_test_minmax = pd.DataFrame(minmax_scaler.transform(X_test), columns = X_train.columns)\n",
    "pred_minmax = gdcv_minmax_model.best_estimator_.predict(X_test_minmax)\n",
    "\n",
    "# plot pred vs actual\n",
    "plt.plot(y_test.values, pred_minmax, c='g', marker='o', linestyle='None')\n",
    "plt.plot([0,3500], [0, 3500], c='r')\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Actual vs Predicted')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Polynomial features\n",
    "\n",
    "Other approach is to create polynomial features. In this case, if the original set of feature is $(x_1, x_2, x_n)$ then the polynomial features with degree 2 are $(1, x_1, x_2, x_n, x_1^2, x_1x_2, x_1x_n, x_2^2, x_2x_n, ...,  x_n^2 \\ldots)$.\n",
    "\n",
    "This can be done by coding or simply using sklearn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# set and fit the scaler\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False).fit(X_train)\n",
    "\n",
    "X_train_poly = pd.DataFrame(poly.transform(X_train), columns = poly.get_feature_names_out(X_train.columns))\n",
    "X_test_poly = pd.DataFrame(poly.transform(X_train), columns = poly.get_feature_names_out(X_train.columns))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train a model using the polynomial features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gscv_poly_model = create_model_with_GSCV(X_train_poly, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters and score can be obtained as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gscv_poly_model.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best score is"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gs_score = gscv_poly_model.score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And, to compare the predictions with the actual values, by plotting them. On a prefect regression, the points would be on the diagonal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_poly = gscv_poly_model.best_estimator_.predict(X_test_poly)\n",
    "\n",
    "# plot pred vs actual\n",
    "plt.plot(y_test.values, pred_poly, c='g', marker='o', linestyle='None')\n",
    "plt.plot([0,3500], [0, 3500], c='r')\n",
    "\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Actual vs Predicted')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalization + Polynomial features\n",
    "\n",
    "Now, let us combine both normalization and polynomial features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set and fit the scaler\n",
    "standard_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# normalize the data\n",
    "X_train_normalized = pd.DataFrame(standard_scaler.transform(X_train), columns = X_train.columns)\n",
    "X_test_normalized = pd.DataFrame(standard_scaler.transform(X_test), columns = X_train.columns)\n",
    "\n",
    "# set the polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=True).fit(X_train_normalized)\n",
    "\n",
    "# create the polynomial features\n",
    "X_train_normalized_poly = pd.DataFrame(poly.transform(X_train_normalized), columns = poly.get_feature_names_out(X_train_normalized.columns))\n",
    "X_test_normalized_poly = pd.DataFrame(poly.transform(X_test_normalized), columns = poly.get_feature_names_out(X_test_normalized.columns))\n",
    "\n",
    "# train the model\n",
    "gscv_normalized_poly_model = create_model_with_GSCV(X_train_normalized_poly, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters and score can be obtained as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get the best score\n",
    "print(f'Best score: {gscv_normalized_poly_model.best_score_}')\n",
    "\n",
    "# get the best parameters\n",
    "print(f'Best parameters: {gscv_normalized_poly_model.best_params_}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And, to compare the predictions with the actual values, by plotting them. On a prefect regression, the points would be on the diagonal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_normalized_poly = gscv_normalized_poly_model.best_estimator_.predict(X_test_normalized_poly)\n",
    "\n",
    "# plot pred vs actual\n",
    "plt.plot(y_test.values, pred_normalized_poly, c='g', marker='o', linestyle='None')\n",
    "plt.plot([0,3500], [0, 3500], c='r')\n",
    "\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Actual vs Predicted')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
