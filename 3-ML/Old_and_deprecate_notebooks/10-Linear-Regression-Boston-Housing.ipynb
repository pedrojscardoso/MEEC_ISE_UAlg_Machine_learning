{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "<h1> Machine Learning </h1>\n",
    "<h2> M. Sc. in Electrical and Computer Engineering </h2>\n",
    "<h3> Instituto Superior de Engenharia / Universidade do Algarve </h3>\n",
    "\n",
    "[MEEC](https://ise.ualg.pt/en/curso/1477) / [ISE](https://ise.ualg.pt) / [UAlg](https://www.ualg.pt)\n",
    "\n",
    "Pedro J. S. Cardoso (pcardoso@ualg.pt)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T10:24:30.613048Z",
     "start_time": "2018-04-19T10:24:30.610057Z"
    }
   },
   "source": [
    "# Linear regression\n",
    "\n",
    "## Ordinary least squares (OLS)\n",
    "\n",
    "Let us generate some data and split it before fitting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wave(n_samples=100):\n",
    "    \"\"\" builds a sample with n_samples in the form y = x + random()\"\"\"\n",
    "    rnd = np.random.RandomState(1)\n",
    "    x = rnd.uniform(-10, 10, size=n_samples)\n",
    "    y_no_noise = x\n",
    "    y = y_no_noise + (rnd.normal(size=len(x)))\n",
    "    x = x.reshape(-1, 1) # reshape to a (n,1) shape\n",
    "    return x, y \n",
    "\n",
    "X, y = make_wave(n_samples=100)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Ordinary least squares Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "print(\"lr.coef_: {}\".format(ols.coef_))\n",
    "print(\"lr.intercept_: {}\".format(ols.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An R^2 of around 0.9 might not be not very bad...\n",
    "\n",
    "(see: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(ols.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ols.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ols.predict(x_test)\n",
    "\n",
    "plt.plot(x_test, y_pred, label='pred')\n",
    "plt.scatter(x_test, y_test, label='test')\n",
    "plt.scatter(x_train, y_train, label='train')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston Housing dataset\n",
    "\n",
    "NOTE: This notebook for recent versions of scikit-learn does not work anymore. \n",
    "See https://scikit-learn.org/1.1/modules/generated/sklearn.datasets.load_boston.html for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "# from mglearn.datasets import load_extended_boston\n",
    "\n",
    "boston = load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boston Housing dataset: dataset has 506 samples and 13 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['House value'] = boston.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying OLS to the Boston data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "LinearRegression fits a linear model with coefficients $w = (w_1, \\ldots, w_p)$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation, i.e., setting $\\hat y = \\sum_i w_i x_i + b$, OLS optimizes  $\\min_{w}||y - Xw||^2_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = boston.data, boston.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    random_state=0, \n",
    "                                                    shuffle=True, \n",
    "                                                    train_size=.7\n",
    "                                                   )\n",
    "\n",
    "ols = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing training set and test set scores, we find that we predict more accurately on the training than in the test set, as expected!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(ols.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ols.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Ridge regression to the Boston data set\n",
    "\n",
    "Recall that, Ridge regression minimizes the objective function: \n",
    "$||y - Xw||^2_2 + \\alpha * ||w||^2_2$\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=1).fit(x_train, y_train)\n",
    "\n",
    "print(\"rr.coef_: {}\".format(rr.coef_))\n",
    "print(\"rr.intercept_: {}\".format(rr.intercept_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(rr.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(rr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing OLS and Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "ridge_scores_train = []\n",
    "ridge_scores_test = []\n",
    "\n",
    "alphas = np.arange(0, 5, .1)\n",
    "\n",
    "ols = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "# ols.score(x_test, y_test)\n",
    "\n",
    "for alpha in alphas:\n",
    "    rr = Ridge(alpha=alpha).fit(x_train, y_train)\n",
    "    ridge_scores_train.append(rr.score(x_train, y_train))\n",
    "    ridge_scores_test.append(rr.score(x_test, y_test))\n",
    "\n",
    "plt.plot(alphas, ols.score(x_train, y_train) * np.ones(len(alphas)), '--', label='OLS - train')\n",
    "plt.plot(alphas, ols.score(x_test, y_test) * np.ones(len(alphas)), '--', label='OLS - test')\n",
    "\n",
    "plt.plot(alphas, ridge_scores_train, label='Ridge - train')\n",
    "plt.plot(alphas, ridge_scores_test, label='Ridge - test')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('alpha')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T12:37:29.810190Z",
     "start_time": "2018-04-19T12:37:29.806187Z"
    }
   },
   "source": [
    "### Exercises\n",
    "\n",
    "Fix $\\alpha = 0.1$ and then investigate how the size of the training dataset affects the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Boston dataset\n",
    "\n",
    "If we look at Boston data we can see that, i.e., different value magnituds appear on columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let us do some data transformation, namely:\n",
    "- scalling\n",
    "- polynomial combinations of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_extended_boston():\n",
    "    from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "\n",
    "    boston = load_boston()\n",
    "    X = boston.data\n",
    "\n",
    "    # Transforms features by scaling each feature to a given range.\n",
    "    X = MinMaxScaler().fit_transform(boston.data)\n",
    "    \n",
    "    # Generate a new feature matrix consisting of all polynomial combinations of the features \n",
    "    # with degree less than or equal to the specified degree. \n",
    "    # For example, if an input sample is two dimensional and of the form [a, b], \n",
    "    # the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
    "    poly_fit = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X = poly_fit.fit_transform(X)\n",
    "    return X, boston.target, poly_fit.get_feature_names_out(boston.feature_names)\n",
    "\n",
    "X, y, feature_names = do_extended_boston()\n",
    "print(X.shape)\n",
    "\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the extended Boston Housing dataset: dataset has 506 samples and 104 derived features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying OLS on the extended Boston dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing training set and test set scores, we find that we predict very accurately\n",
    "on the training set (overfitting?), but the R2 on the test set is much worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(ols.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ols.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Ridge regression on the extended boston data set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=.1).fit(x_train, y_train)\n",
    "\n",
    "print(\"rr.coef_: {}\".format(rr.coef_))\n",
    "print(\"rr.intercept_: {}\".format(rr.intercept_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(rr.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(rr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing OLS and Ridge on the Boston extended dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "ridge_scores_train = []\n",
    "ridge_scores_test = []\n",
    "\n",
    "alphas = np.arange(0, 5, .1)\n",
    "\n",
    "ols = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "# ols.score(x_test, y_test)\n",
    "\n",
    "for alpha in alphas:\n",
    "    rr = Ridge(alpha=alpha).fit(x_train, y_train)\n",
    "    ridge_scores_train.append(rr.score(x_train, y_train))\n",
    "    ridge_scores_test.append(rr.score(x_test, y_test))\n",
    "\n",
    "plt.plot(alphas, ols.score(x_train, y_train) * np.ones(len(alphas)), '--', label='OLS - train')\n",
    "plt.plot(alphas, ols.score(x_test, y_test) * np.ones(len(alphas)), '--', label='OLS - test')\n",
    "\n",
    "plt.plot(alphas, ridge_scores_train, label='Ridge - train')\n",
    "plt.plot(alphas, ridge_scores_test, label='Ridge - test')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('alpha')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T12:37:29.810190Z",
     "start_time": "2018-04-19T12:37:29.806187Z"
    }
   },
   "source": [
    "### Exercises\n",
    "\n",
    "1. Fix $\\alpha = 0.1$ and then investigate how the size of the training dataset affects the score\n",
    "1. Do a similar study but with \"scalling\" and \"polynomial combinations of the features\" done individually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Lasso Regression on the extended Boston dataset)\n",
    "Just to recall, the optimization objective for Lasso is:\n",
    "$\\frac{1}{2  n_{samples}}  ||y - Xw||^2_2 + \\alpha  ||w||_1$\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Lasso(alpha=.01).fit(x_train, y_train)\n",
    "\n",
    "print(\"rr.coef_: {}\".format(lr.coef_))\n",
    "print(\"rr.intercept_: {}\".format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(lr.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(x_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lr.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, keeping the default parameters, Lasso does quite badly, both on the training and the test set. This\n",
    "indicates that we are underfitting, and we find that it used only 4 of the 105 features. \n",
    "\n",
    "But if we change the alpha parameter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Lasso(alpha=0.01, max_iter=10000).fit(x_train, y_train)\n",
    "\n",
    "print(\"lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(x_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lr.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "ridge_scores_train = []\n",
    "ridge_scores_test = []\n",
    "\n",
    "lasso_scores_train = []\n",
    "lasso_scores_test = []\n",
    "\n",
    "alphas = np.arange(0, 2, .01)\n",
    "\n",
    "ols = LinearRegression().fit(x_train, y_train)\n",
    "ols.score(x_test, y_test)\n",
    "\n",
    "for alpha in alphas:\n",
    "    rr = Ridge(alpha=alpha).fit(x_train, y_train)\n",
    "    ridge_scores_train.append(rr.score(x_train, y_train))\n",
    "    ridge_scores_test.append(rr.score(x_test, y_test))\n",
    "\n",
    "    lr = Lasso(alpha=alpha, max_iter=100000).fit(x_train, y_train)\n",
    "    lasso_scores_train.append(lr.score(x_train, y_train))\n",
    "    lasso_scores_test.append(lr.score(x_test, y_test))\n",
    "\n",
    "plt.plot(alphas, ols.score(x_train, y_train) * np.ones(len(alphas)), '--', label='OLS - train')\n",
    "plt.plot(alphas, ols.score(x_test, y_test) * np.ones(len(alphas)), '--', label='OLS - test')\n",
    "\n",
    "plt.plot(alphas, ridge_scores_train, label='Ridge - train')\n",
    "plt.plot(alphas, ridge_scores_test, label='Ridge - test')\n",
    "\n",
    "plt.plot(alphas, lasso_scores_train, label='Lasso - train')\n",
    "plt.plot(alphas, lasso_scores_test, label='Lasso - test')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('alpha')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lower alpha allowed us to fit a more complex model, which worked better on the training and test data. The performance is slightly better than using Ridge, and we are using only 33 of the 105 features. This makes this model potentially easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the diabetes dataset. Note that each of the 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
    "\n",
    "\n",
    "* use the first 400 observations to train a linear regression model and predict/verify the disease progression for the remaining \n",
    "\n",
    "* For a patiente with values [-0.02367725, -0.04464164,  0.04552903,  0.09072977, -0.01808039, -0.03544706,  0.07072993, -0.03949338, -0.03452372, -0.00936191] predict the disease progression one year after baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diab = load_diabetes()\n",
    "\n",
    "diab\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "422px",
    "width": "439px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
