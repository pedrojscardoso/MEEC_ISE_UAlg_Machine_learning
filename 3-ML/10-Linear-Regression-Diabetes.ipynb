{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "<h1> Machine Learning </h1>\n",
    "<h2> M. Sc. in Electrical and Computer Engineering </h2>\n",
    "<h3> Instituto Superior de Engenharia / Universidade do Algarve </h3>\n",
    "\n",
    "[MEEC](https://ise.ualg.pt/en/curso/1477) / [ISE](https://ise.ualg.pt) / [UAlg](https://www.ualg.pt)\n",
    "\n",
    "Pedro J. S. Cardoso (pcardoso@ualg.pt)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T10:24:30.613048Z",
     "start_time": "2018-04-19T10:24:30.610057Z"
    }
   },
   "source": [
    "# Linear regression\n",
    "\n",
    "## Ordinary least squares (OLS)\n",
    "\n",
    "Let us generate some data and split it before fitting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wave(n_samples=100):\n",
    "    \"\"\" builds a sample with n_samples in the form y = x + random()\"\"\"\n",
    "    rnd = np.random.RandomState(1)\n",
    "    x = rnd.uniform(-10, 10, size=n_samples)\n",
    "    y_no_noise = x\n",
    "    y = y_no_noise + (rnd.normal(size=len(x)))\n",
    "    x = x.reshape(-1, 1) # reshape to a (n,1) shape\n",
    "    return x, y \n",
    "\n",
    "X, y = make_wave(n_samples=100)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Ordinary least squares Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "print(\"lr.coef_: {}\".format(ols.coef_))\n",
    "print(\"lr.intercept_: {}\".format(ols.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An R^2 of around 0.9 might not be not very bad...\n",
    "\n",
    "(see: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(ols.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ols.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ols.predict(x_test)\n",
    "\n",
    "plt.plot(x_test, y_pred, label='pred')\n",
    "plt.scatter(x_test, y_test, label='test')\n",
    "plt.scatter(x_train, y_train, label='train')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diab = load_diabetes()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset description is:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(diab.DESCR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset has 442 samples and 10 features. The features are:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The target is progression of the disease. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "diab.target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us place the data in a pandas dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(diab.data, columns=diab.feature_names)\n",
    "df['evolution'] = diab.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can use Pandas to explore the dataset a bit more in detail (what conclusions can you draw from the data? why is data values between 0 and 1? suggestion: see the dataset description) "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying OLS to the Diabetes data set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "LinearRegression fits a linear model with coefficients $w = (w_1, \\ldots, w_p)$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation, i.e., setting $\\hat y = \\sum_i w_i x_i + b$, OLS optimizes  $\\min_{w}||y - Xw||^2_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = diab.data, diab.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    shuffle=True, \n",
    "                                                    train_size=.75,\n",
    "                                                    random_state=42\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ols = LinearRegression().fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing training set and test set scores, we find that we predict more accurately on the training than in the test set, as expected!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(ols.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ols.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also compute other metrics, such as the mean squared error and the mean absolute error:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "y_pred = ols.predict(x_test) \n",
    "\n",
    "print(\"Mean squared error: {:.2f}\".format(mean_squared_error(y_test, y_pred)))\n",
    "print(\"Mean absolute error: {:.2f}\".format(mean_absolute_error(y_test, y_pred)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying Ridge regression to the Diabetes data set\n",
    "\n",
    "Recall that, Ridge regression minimizes the objective function:\n",
    "$||y - Xw||^2_2 + \\alpha * ||w||^2_2$\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=1).fit(x_train, y_train)\n",
    "\n",
    "print(\"rr.coef_: {}\".format(rr.coef_))\n",
    "print(\"rr.intercept_: {}\".format(rr.intercept_))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(rr.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(rr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing OLS and Ridge\n",
    "\n",
    "Let us compare the OLS and Ridge regression models on the Diabetes dataset. For the Ridge regression model, we will vary the value of the regularization parameter $\\alpha$. From the plot below, we can see that the regularization parameter $\\alpha$ allows to slightly improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "ridge_scores_train = []\n",
    "ridge_scores_test = []\n",
    "\n",
    "alphas = np.arange(0, 2, 0.01)\n",
    "\n",
    "ols = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "for alpha in alphas:\n",
    "    rr = Ridge(alpha=alpha, ).fit(x_train, y_train)\n",
    "    ridge_scores_train.append(rr.score(x_train, y_train))\n",
    "    ridge_scores_test.append(rr.score(x_test, y_test))\n",
    "\n",
    "plt.plot(alphas, ols.score(x_train, y_train) * np.ones(len(alphas)), '--', label='OLS - train')\n",
    "plt.plot(alphas, ols.score(x_test, y_test) * np.ones(len(alphas)), '--', label='OLS - test')\n",
    "\n",
    "plt.plot(alphas, ridge_scores_train, label='Ridge - train')\n",
    "plt.plot(alphas, ridge_scores_test, label='Ridge - test')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('alpha')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T12:37:29.810190Z",
     "start_time": "2018-04-19T12:37:29.806187Z"
    }
   },
   "source": [
    "|### Exercises\n",
    "\n",
    "Fix $\\alpha = 0.1$ and then investigate how the size of the training dataset affects the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Diabetes dataset\n",
    "\n",
    "If we look at  data we can see that, i.e., different value magnituds appear on columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let us do some data transformation, namely:\n",
    "- scalling\n",
    "- polynomial combinations of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_extended_diab():\n",
    "    from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "\n",
    "    diab = load_diabetes()\n",
    "    X = diab.data\n",
    "\n",
    "    # Transforms features by scaling each feature to a given range.\n",
    "    X = MinMaxScaler().fit_transform(diab.data)\n",
    "    \n",
    "    # Generate a new feature matrix consisting of all polynomial combinations of the features \n",
    "    # with degree less than or equal to the specified degree. \n",
    "    # For example, if an input sample is two dimensional and of the form [a, b], \n",
    "    # the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
    "    poly_fit = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X = poly_fit.fit_transform(X)\n",
    "    \n",
    "    return X, diab.target, poly_fit.get_feature_names_out(diab.feature_names)\n",
    "\n",
    "X, y, feature_names = do_extended_diab()\n",
    "print(X.shape)\n",
    "\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the extended California housing dataset: dataset has 442 samples and 65 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying OLS on the extended Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing training set and test set scores, we find that we predict very accurately on the training set (overfitting?), but the R2 on the test set is worse... as expected!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(ols.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ols.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Ridge regression on the extended Diabetes data set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=1).fit(x_train, y_train)\n",
    "\n",
    "print(\"rr.coef_: {}\".format(rr.coef_))\n",
    "print(\"rr.intercept_: {}\".format(rr.intercept_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(rr.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(rr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing OLS and Ridge on the California housing extended dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "ridge_scores_train = []\n",
    "ridge_scores_test = []\n",
    "\n",
    "alphas = np.arange(0, 2, 0.01)\n",
    "\n",
    "ols = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "# ols.score(x_test, y_test)\n",
    "\n",
    "for alpha in alphas:\n",
    "    rr = Ridge(alpha=alpha).fit(x_train, y_train)\n",
    "    ridge_scores_train.append(rr.score(x_train, y_train))\n",
    "    ridge_scores_test.append(rr.score(x_test, y_test))\n",
    "\n",
    "plt.plot(alphas, ols.score(x_train, y_train) * np.ones(len(alphas)), '--', label='OLS - train')\n",
    "plt.plot(alphas, ols.score(x_test, y_test) * np.ones(len(alphas)), '--', label='OLS - test')\n",
    "\n",
    "plt.plot(alphas, ridge_scores_train, label='Ridge - train')\n",
    "plt.plot(alphas, ridge_scores_test, label='Ridge - test')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('alpha')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T12:37:29.810190Z",
     "start_time": "2018-04-19T12:37:29.806187Z"
    }
   },
   "source": [
    "### Exercises\n",
    "\n",
    "1. Fix $\\alpha = 0.1$ and then investigate how the size of the training dataset affects the score\n",
    "1. Do a similar study but with \"scalling\" and \"polynomial combinations of the features\" done individually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Lasso Regression on the extended Diabetes dataset\n",
    "Just to recall, the optimization objective for Lasso is:\n",
    "$\\frac{1}{2  n_{samples}}  ||y - Xw||^2_2 + \\alpha  ||w||_1$\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Lasso().fit(x_train, y_train)\n",
    "\n",
    "print(\"rr.coef_: {}\".format(lr.coef_))\n",
    "print(\"rr.intercept_: {}\".format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(lr.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(x_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lr.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, keeping the default parameters, Lasso does not so well, both on the training and the test set. This indicates that we are underfitting, and we find that it used only 4 of the 44 features.\n",
    "\n",
    "But if we change the alpha parameter some improvement is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Lasso(alpha=0.2, max_iter=10000).fit(x_train, y_train)\n",
    "\n",
    "print(\"lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(x_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lr.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "ridge_scores_train = []\n",
    "ridge_scores_test = []\n",
    "\n",
    "lasso_scores_train = []\n",
    "lasso_scores_test = []\n",
    "\n",
    "alphas = np.arange(0, 2, .01)\n",
    "\n",
    "ols = LinearRegression().fit(x_train, y_train)\n",
    "ols.score(x_test, y_test)\n",
    "\n",
    "for alpha in alphas:\n",
    "    rr = Ridge(alpha=alpha).fit(x_train, y_train)\n",
    "    ridge_scores_train.append(rr.score(x_train, y_train))\n",
    "    ridge_scores_test.append(rr.score(x_test, y_test))\n",
    "\n",
    "    lr = Lasso(alpha=alpha, max_iter=100000).fit(x_train, y_train)\n",
    "    lasso_scores_train.append(lr.score(x_train, y_train))\n",
    "    lasso_scores_test.append(lr.score(x_test, y_test))\n",
    "\n",
    "plt.plot(alphas, ols.score(x_train, y_train) * np.ones(len(alphas)), '--', label='OLS - train')\n",
    "plt.plot(alphas, ols.score(x_test, y_test) * np.ones(len(alphas)), '--', label='OLS - test')\n",
    "\n",
    "plt.plot(alphas, ridge_scores_train, label='Ridge - train')\n",
    "plt.plot(alphas, ridge_scores_test, label='Ridge - test')\n",
    "\n",
    "plt.plot(alphas, lasso_scores_train, label='Lasso - train')\n",
    "plt.plot(alphas, lasso_scores_test, label='Lasso - test')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('alpha')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lower alpha allowed us to fit a more complex model, which worked better on the training and test data. The performance is slightly better than using Ridge, and we are using only some of the 44 features. This makes this model potentially easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercises\n",
    "\n",
    "Make a similar analysis for wine dataset, that you can find here: https://archive.ics.uci.edu/dataset/186/wine+quality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "422px",
    "width": "439px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
